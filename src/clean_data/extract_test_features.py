# NOTE:
# Documentation comments in this file is generated by ChatGPT

from pathlib import Path

import pandas
import spacy.util
from textblob import TextBlob
from textstat import textstat

import re
from markdown import markdown
from bs4 import BeautifulSoup

root_dir = Path(__file__).parent.parent.parent
nlp = None


def markdown_to_text(text):
    """
    Convert Markdown-formatted text to plain text.

    Args:
        text (str): Markdown-formatted text.

    Returns:
        A string of plain text.

    """
    # Convert Markdown to HTML.
    html = markdown(text)

    # Parse HTML and return plain text.
    bs = BeautifulSoup(html, features="html.parser")
    return bs.get_text()


def get_urls(text):
    """
    Extract URLs and DOIs from Markdown-formatted text.

    Args:
        text (str): Markdown-formatted text.

    Returns:
        A tuple containing the total number of URLs found, a list of all URLs found, the number of DOI URLs found, and a list of DOI URLs.

    """
    # Find all URLs in the text using a regular expression.
    urls = re.findall(r"\[.*\]\((.*)\)", markdown_to_text(text))

    # Filter the list of URLs to only include DOI URLs.
    doi_urls = [u for u in urls if "doi.org" in u]

    # Return a tuple containing the relevant counts and lists.
    return len(urls), urls, len(doi_urls), doi_urls


def get_isbns(text):
    """
    Extract ISBNs from Markdown-formatted text.

    Args:
        text (str): Markdown-formatted text.

    Returns:
        A tuple containing the total number of ISBNs found and a list of all ISBNs found.

    """
    # Define a regular expression pattern to find ISBNs.
    pattern = r"ISBN(?:-?1[03])?:\s?([-\d ]+)"

    # Find all ISBNs in the text using the regular expression.
    isbns = re.findall(pattern, markdown_to_text(text), re.MULTILINE)

    # Return a tuple containing the relevant counts and lists.
    return len(isbns), isbns


def get_issns(text):
    """
    Extract ISSNs from Markdown-formatted text.

    Args:
        text (str): Markdown-formatted text.

    Returns:
        A tuple containing the total number of ISSNs found and a list of all ISSNs found.

    """
    # Define a regular expression pattern to find ISSNs.
    pattern = r"ISSN:?\s?([-\d ]+)"

    # Find all ISSNs in the text using the regular expression.
    issns = re.findall(pattern, markdown_to_text(text), re.MULTILINE)

    # Return a tuple containing the relevant counts and lists.
    return len(issns), issns


def prepare_spacy():
    """
    Download and load a spaCy language model for English.

    Returns:
        None

    Notes:
        This function sets a global variable `nlp` to the loaded spaCy language model, which can be used for further text processing.
    """
    global nlp
    model_name = "en_core_web_lg"
    # Check if the spaCy language model is installed, and download it if necessary.
    if not spacy.util.is_package(model_name):
        spacy.cli.download(model_name)
    # Load the spaCy language model into a global variable.
    nlp = spacy.load(model_name)


def count_noun_verb_phrases(text):
    """
    Count the number of noun phrases and verb phrases in a given text.

    Args:
        text (str): The text to analyze.

    Returns:
        A tuple containing the number of noun phrases and the number of verb phrases in the input text.

    """
    global nlp
    doc = nlp(markdown_to_text(text))

    # Use list comprehension and generator expression for improved efficiency
    noun_phrase_count = sum(1 for chunk in doc.noun_chunks)
    verb_phrase_count = sum(1 for token in doc if token.pos_ == "VERB")

    return noun_phrase_count, verb_phrase_count


def get_readability_scores(df):
    """
    Calculate the Flesch readability score for each section of text in a pandas DataFrame, as well as a composite score
    for the entire DataFrame.

    Args:
        df (pandas.DataFrame): A DataFrame containing text data.

    Returns:
        A DataFrame with new columns added for the Flesch readability score of each section of text, as well as a
        composite score for the entire DataFrame.

    """
    section_columns = [
        "1. Summary of the impact",
        "2. Underpinning research",
        "3. References to the research",
        "4. Details of the impact",
        "5. Sources to corroborate the impact",
    ]
    for i, s in zip(range(1, 6), section_columns):
        # Apply the textstat.flesch_reading_ease function to each cell in the current section column of the DataFrame.
        df[f"s{i}_flesch_score"] = df[s].apply(
            lambda x: textstat.flesch_reading_ease(markdown_to_text(x))
        )
    # Calculate the Flesch readability score for the entire DataFrame by concatenating the text of each section
    # and applying the textstat.flesch_reading_ease function to the concatenated text.
    df["flesch_score"] = df.apply(
        lambda x: textstat.flesch_reading_ease(
            "\n".join([x[s] for s in section_columns])
        ),
        axis=1,
    )
    # Return the DataFrame with the new columns added.
    return df


def extract_urls_ids(df):
    """
    Extract URLs, DOIs, ISBNs, and ISSNs from each section of text in a pandas DataFrame.

    Args:
        df (pandas.DataFrame): A DataFrame containing text data.

    Returns:
        A DataFrame with new columns added for the count and values of URLs, DOIs, ISBNs, and ISSNs in each section of text.

    """
    section_columns = [
        "1. Summary of the impact",
        "2. Underpinning research",
        "3. References to the research",
        "4. Details of the impact",
        "5. Sources to corroborate the impact",
    ]
    for i, s in zip(range(1, 6), section_columns):
        # Extract the count and values of URLs from each cell in the current section column of the DataFrame.
        df[[f"s{i}_urls_count", f"s{i}_urls", f"s{i}_dois_count", f"s{i}_dois"]] = df[
            s
        ].apply(lambda x: pandas.Series(get_urls(x)))
        # Extract the count and values of ISBNs from each cell in the current section column of the DataFrame.
        df[[f"s{i}_isbns_count", f"s{i}_isbns"]] = df[s].apply(
            lambda x: pandas.Series(get_isbns(x))
        )
        # Extract the count and values of ISSNs from each cell in the current section column of the DataFrame.
        df[[f"s{i}_issns_count", f"s{i}_issns"]] = df[s].apply(
            lambda x: pandas.Series(get_issns(x))
        )
    # Return the DataFrame with the new columns added.
    return df


def get_pos_features(df):
    """
    Calculate the number of noun phrases and verb phrases in each section of text in a pandas DataFrame.

    Args:
        df (pandas.DataFrame): A DataFrame containing text data.

    Returns:
        A DataFrame with new columns added for the number of noun phrases and verb phrases in each section of text.
    """
    section_columns = [
        "1. Summary of the impact",
        "2. Underpinning research",
        "3. References to the research",
        "4. Details of the impact",
        "5. Sources to corroborate the impact",
    ]
    for i, s in zip(range(1, 6), section_columns):
        # Apply the count_noun_verb_phrases function to each cell in the current section column of the DataFrame.
        df[[f"s{i}_np_count", f"s{i}_vp_count"]] = df[s].apply(
            lambda x: pandas.Series(count_noun_verb_phrases(x))
        )
    # Return the DataFrame with the new columns added.
    return df


def get_sentiment_score(text):
    """
    Get the sentiment polarity score of a text using TextBlob.

    Args:
        text (str): A string of text to analyze.

    Returns:
        A float representing the sentiment polarity score of the input text, in the range [-1.0, 1.0].
        A score of -1.0 indicates extremely negative sentiment, a score of 1.0 indicates extremely positive sentiment,
        and a score of 0.0 indicates neutral sentiment.

    Raises:
        Nothing.

    Example:
        N/A
    """
    # Convert the input Markdown text to plain text using the markdown_to_text function.
    plain_text = markdown_to_text(text)

    # Create a TextBlob object from the plain text.
    blob = TextBlob(plain_text)

    # Get the sentiment polarity score of the TextBlob object.
    sentiment_score = blob.sentiment_assessments.polarity

    # Return the sentiment polarity score.
    return sentiment_score


def get_sentiment_scores(df):
    """
    Calculate the sentiment score for each section of text in a pandas DataFrame, as well as the overall sentiment score
    for the entire DataFrame.

    Args:
        df (pandas.DataFrame): A DataFrame containing text data.

    Returns:
        A DataFrame with new columns added for the sentiment score of each section of text, as well as the overall
        sentiment score of the DataFrame.

    """
    section_columns = [
        "1. Summary of the impact",
        "2. Underpinning research",
        "3. References to the research",
        "4. Details of the impact",
        "5. Sources to corroborate the impact",
    ]
    # Calculate the sentiment score for each section of text in the DataFrame.
    for i, s in zip(range(1, 6), section_columns):
        df[f"s{i}_sentiment_score"] = df[s].apply(lambda x: get_sentiment_score(x))
    # Calculate the overall sentiment score for the DataFrame by concatenating the text from each section and applying
    # the get_sentiment_score function.
    df["sentiment_score"] = df.apply(
        lambda x: get_sentiment_score("\n".join([x[s] for s in section_columns])),
        axis=1,
    )
    # Return the DataFrame with the new columns added.
    return df


def main():
    df = pandas.read_pickle(
        root_dir.joinpath("data", "merged", "merged_ref_data_exc_output.pkl")
    )
    # extra_data = toml.load(
    #     root_dir.joinpath("src", "clean_data", "extra_data", "ics_country_funder.toml")
    # )
    # df_extra = pandas.DataFrame(extra_data["impact case study"])
    # df = pandas.merge(df, df_extra, how="left", on="REF impact case study identifier")
    # df = extract_urls_ids(df)
    # df = get_readability_scores(df)
    prepare_spacy()
    df = get_pos_features(df)
    df = get_sentiment_scores(df)
    dir_text = root_dir.joinpath("data", "with_text_features")
    dir_text.mkdir(parents=True, exist_ok=True)
    df.to_pickle(dir_text.joinpath("merged_with_text_features.pkl"))
    return df
